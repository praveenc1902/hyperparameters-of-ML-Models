SVM offers very high accuracy compared to other classifiers such as logistic regression, and decision trees. 

The classifier separates data points using a hyperplane with the largest amount of margin. That's why an SVM classifier is also known as a discriminative classifier
margin --> distance b/w the line and nearest data points 

SVM constructs a hyperplane in multidimensional space to separate different classes. 
SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. 
The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.


Support Vectors :: 
Support vectors are the data points, which are closest to the hyperplane. 
These points will define the separating line better by calculating margins.

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Hyper Parameters
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

KERNALS ::
Some problems canâ€™t be solved using linear hyperplane,
In such situation, SVM uses a kernel trick to transform the input space to a higher dimensional space.
The data points are plotted on the x-axis and z-axis (Z is the squared sum of both x and y: z=x^2=y^2). 
Now you can easily segregate these points using linear separation.

C ::
Its a reguralization parameter. is the penalty parameter, which represents misclassification or error term.
A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.

Gamma ::
a low value of gamma considers only nearby points in calculating the separation line, while the a value of gamma
considers all the data points in the calculation of the separation line
